{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "# preprocessing imports\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ICLEVRLayoutDataset(Dataset):\n",
    "    def __init__(self, cache_text_path=\"/users/jason wu/downloads/GeNeVA-v1/i-CLEVR/cache_text/\", cache_scene_path=\"/users/jason wu/downloads/GeNeVA-v1/i-CLEVR/cache_scenes/\", max_seq_len=5, split=\"train\"):\n",
    "        super(ICLEVRLayoutDataset, self).__init__()\n",
    "        self.cache_text_path = cache_text_path\n",
    "        self.cache_scene_path = cache_scene_path\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.split = split\n",
    "        \n",
    "        self.keys = [f.split(\".\")[0] for f in os.listdir(cache_text_path) if \".pkl\" in f and split in f]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.keys)\n",
    "        key = self.keys[idx]\n",
    "        key_file = key + \".pkl\"\n",
    "        txt_path = os.path.join(self.cache_text_path, key_file)\n",
    "        scene_path = os.path.join(self.cache_scene_path, key_file)\n",
    "        txt_data = joblib.load(txt_path)\n",
    "        scene_data = joblib.load(scene_path)\n",
    "        txt_data = torch.stack(txt_data, dim=0)\n",
    "        orig_len = txt_data.shape[0]\n",
    "        txt_data_padded = torch.zeros(self.max_seq_len, txt_data.shape[-1])\n",
    "        txt_data_padded[:orig_len, :] = txt_data\n",
    "        last_scene = torch.FloatTensor(scene_data[0])\n",
    "        target_scene = torch.FloatTensor(scene_data[1])\n",
    "        last_scene_padded = torch.zeros(self.max_seq_len, target_scene.shape[-1])\n",
    "        if orig_len > 1:\n",
    "            last_scene_padded[:last_scene.shape[0], :] = last_scene\n",
    "        \n",
    "        target_scene_padded = torch.zeros(self.max_seq_len, target_scene.shape[-1])\n",
    "        target_scene_padded[:target_scene.shape[0], :] = target_scene\n",
    "#         if last_scene.shape[0] == 0:\n",
    "#             last_scene_padded = torch.zeros(target_scene.shape[-1])\n",
    "#         else:\n",
    "#             last_scene_padded = last_scene[last_scene.shape[0] - 1]\n",
    "#         target_scene_padded = target_scene[target_scene.shape[0] - 1]\n",
    "        element_dict = {}\n",
    "        element_dict['txt'] = txt_data_padded\n",
    "        element_dict['seq_len'] = orig_len\n",
    "        element_dict['last_scene'] = last_scene_padded\n",
    "        element_dict['target_scene'] = target_scene_padded\n",
    "        element_dict['idx'] = idx\n",
    "#         return txt_data_padded, last_scene_padded, target_scene_padded\n",
    "        return element_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ICLEVRLayoutDataset(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLEVRLayoutDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, num_workers=0, **kwargs):\n",
    "        super(ICLEVRLayoutDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        # instantiate different splits of dataset\n",
    "        self.train_dataset = ICLEVRLayoutDataset(split=\"train\")\n",
    "        self.test_dataset = ICLEVRLayoutDataset(split=\"test\")\n",
    "        self.dataloader_obj = DataLoader\n",
    "\n",
    "    @staticmethod\n",
    "    def add_dataset_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "        parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "        return parser\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader_obj(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.dataloader_obj(self.test_dataset, batch_size=1, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ICLEVRLayoutDataModule(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLEVRLayoutGen(pl.LightningModule):\n",
    "    def __init__(self, num_heads=2, hidden_size=32, num_layers=2, dropout=0.25, lr=1e-4, num_shapes=3, num_colors=8, weight_decay=0, query_dim=100, scene_dim=13, **kwargs):\n",
    "        super(ICLEVRLayoutGen, self).__init__()\n",
    "        # handle argparse format of booleans\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.query_encoder = nn.Sequential(nn.Linear(query_dim, hidden_size), nn.LayerNorm(hidden_size))\n",
    "        self.scene_encoder = nn.Sequential(nn.Linear(scene_dim, hidden_size), nn.LayerNorm(hidden_size))\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, dropout)\n",
    "        transformer_layers = nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layers, num_layers)\n",
    "        self.scene_decoder = nn.Linear(hidden_size, scene_dim)\n",
    "    \n",
    "    def forward(self, queries, scenes, paddingMask=None):\n",
    "        query_hidden = self.query_encoder(queries)\n",
    "        scene_hidden = self.scene_encoder(scenes)\n",
    "        embedded = query_hidden + scene_hidden\n",
    "        embedded *= np.sqrt(self.hparams.hidden_size)\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        embedded = self.transformer(embedded, src_key_padding_mask=paddingMask)\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        return self.scene_decoder(embedded)\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batchSize = batch['txt'].shape[0]\n",
    "        maxSeqLen = batch['txt'].shape[1]\n",
    "        screenLen = batch['seq_len']\n",
    "        \n",
    "        sceneTxt = batch['txt']\n",
    "        sceneLast = batch['last_scene']\n",
    "        sceneTarget = batch['target_scene']\n",
    "        \n",
    "        sceneLast[:, :, 0] /= 320\n",
    "        sceneLast[:, :, 1] /= 240\n",
    "        sceneTarget[:, :, 0] /= 320\n",
    "        sceneTarget[:, :, 1] /= 240\n",
    "        \n",
    "        _, targetShapeInd = sceneTarget[:, :, 10:].max(dim=-1)\n",
    "        _, targetColorInd = sceneTarget[:, :, 2:10].max(dim=-1)\n",
    "        \n",
    "        # generate padding mask\n",
    "        paddingMask = torch.arange(maxSeqLen, device=self.device).repeat(batchSize, 1) < screenLen.reshape(-1, 1).repeat(1, maxSeqLen)\n",
    "        # invert padding mask - src_key_padding_mask uses True to mask\n",
    "        paddingMask = ~paddingMask\n",
    "        res = self.forward(sceneTxt, sceneLast, paddingMask=paddingMask)\n",
    "        \n",
    "        paddingMask = ~paddingMask\n",
    "        preds = res[paddingMask]\n",
    "        targetScene = sceneTarget[paddingMask]\n",
    "        \n",
    "        loss_pos = F.l1_loss(preds[:, :2], targetScene[:, :2])\n",
    "        loss_shape = F.cross_entropy(preds[:, 10:].reshape(-1, 3), targetShapeInd[paddingMask].reshape(-1))\n",
    "        loss_color = F.cross_entropy(preds[:, 2:2+8].reshape(-1, 8), targetColorInd[paddingMask].reshape(-1))\n",
    "        \n",
    "#         print(\"res\", res[:, :, 5:].max(dim=-1)[1], \"target\", targetColorInd)\n",
    "        \n",
    "        loss = loss_pos + loss_shape + loss_color\n",
    "        \n",
    "        metrics = {'loss': loss, 'log': {'loss': loss, 'loss_pos': loss_pos, 'loss_shape': loss_shape, 'loss_color': loss_color}}\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay, betas=(0.5, 0.999))\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ICLEVRLayoutGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(callbacks=[ModelCheckpoint(filepath=\"{epoch:02d}\")])\n",
    "\n",
    "# trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ICLEVRLayoutGen.load_from_checkpoint(\"epoch=09.ckpt\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "# initialize the word embeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_txt(t):\n",
    "    sentence = Sentence(t)\n",
    "    document_embeddings.embed(sentence)\n",
    "    return sentence.embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = embed_txt(\"add a red cube in the center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_in = txt.unsqueeze(0).unsqueeze(0)\n",
    "scene_in = torch.zeros(1, 1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100]) torch.Size([1, 1, 13])\n"
     ]
    }
   ],
   "source": [
    "print(txt_in.shape, scene_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4808,  0.4759,  9.1052, -0.2270, -0.0506, -0.0953, -3.2306,\n",
       "          -1.6359, -3.9340, -3.6620, -4.6355,  6.3630, -2.5755]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(txt_in, scene_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'idx':[], 'pos': [], 'color_idx':[], 'shape_idx':[]}\n",
    "\n",
    "for batch in data.test_dataloader():\n",
    "    batchSize = batch['txt'].shape[0]\n",
    "    maxSeqLen = batch['txt'].shape[1]\n",
    "    screenLen = batch['seq_len']\n",
    "\n",
    "    sceneTxt = batch['txt']\n",
    "    sceneLast = batch['last_scene']\n",
    "    sceneTarget = batch['target_scene']\n",
    "\n",
    "    sceneLast[:, :, 0] /= 320\n",
    "    sceneLast[:, :, 1] /= 240\n",
    "    sceneTarget[:, :, 0] /= 320\n",
    "    sceneTarget[:, :, 1] /= 240\n",
    "\n",
    "    _, targetShapeInd = sceneTarget[:, :, 10:].max(dim=-1)\n",
    "    _, targetColorInd = sceneTarget[:, :, 2:10].max(dim=-1)\n",
    "\n",
    "    # generate padding mask\n",
    "    paddingMask = torch.arange(maxSeqLen, device=\"cpu\").repeat(batchSize, 1) < screenLen.reshape(-1, 1).repeat(1, maxSeqLen)\n",
    "    # invert padding mask - src_key_padding_mask uses True to mask\n",
    "    paddingMask = ~paddingMask\n",
    "    res = model.forward(sceneTxt, sceneLast, paddingMask=paddingMask)\n",
    "\n",
    "    paddingMask = ~paddingMask\n",
    "    preds = res[paddingMask]\n",
    "    targetScene = sceneTarget[paddingMask]\n",
    "    pred_pos = preds[:, :2]\n",
    "    pred_shape = preds[:, 10:]\n",
    "    pred_color = preds[:, 2:10]\n",
    "    pred_idx = batch['idx']\n",
    "    \n",
    "    res_dict['idx'].append(pred_idx)\n",
    "    res_dict['pos'].append(pred_pos)\n",
    "    res_dict['shape_idx'].append(pred_shape.max(dim=-1)[1])\n",
    "    res_dict['color_idx'].append(pred_color.max(dim=-1)[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 2, 4, 5]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f.item() for f in res_dict['color_idx'][44]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['res_dict_raw.pkl']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(res_dict, \"res_dict_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict['key'] = []\n",
    "res_dict['shape'] = []\n",
    "res_dict['color'] = []\n",
    "colormap={0:'red',1:'green',2:'blue',3:'cyan',4:'brown',5:'gray',6:'purple',7:'yellow'}\n",
    "shapemap={0:'sphere', 1:'cube', 2:'cylinder'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(res_dict['idx'])):\n",
    "    res_dict['key'].append(ds.keys[res_dict['idx'][i]])\n",
    "    res_dict['shape'].append([shapemap[f.item()] for f in res_dict['shape_idx'][i]])\n",
    "    res_dict['color'].append([colormap[f.item()] for f in res_dict['color_idx'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['res_dict_all.pkl']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(res_dict, \"res_dict_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLEVR_test_000001_0'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['key'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
